{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "from datasets import load_from_disk\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "PILE_DOMAINS = ['ArXiv', 'BookCorpus2', 'Books3', 'DM Mathematics', 'Enron Emails', 'EuroParl', 'FreeLaw', 'Github', 'Gutenberg (PG-19)', 'HackerNews', 'NIH ExPorter', 'OpenSubtitles', 'OpenWebText2', 'PhilPapers', 'Pile-CC', 'PubMed Abstracts', 'PubMed Central', 'StackExchange', 'USPTO Backgrounds', 'Ubuntu IRC', 'Wikipedia (en)', 'YoutubeSubtitles']\n",
    "SLIM_DOMAINS = ['RedPajamaCommonCrawl', 'RedPajamaC4', 'RedPajamaGithub', 'RedPajamaWikipedia', 'RedPajamaBook', 'RedPajamaArXiv', 'RedPajamaStackExchange']\n",
    "preprocessed_dir = \"/home/wth/My_codes/doremi/data/slim_preprocessed/preprocessed\"\n",
    "preprocessed_dir = Path(preprocessed_dir) / 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/wth/My_codes/doremi/tokenizer\")\n",
    "nopack = False\n",
    "\n",
    "def process_shard(shard_dir):\n",
    "    curr_count = 0\n",
    "    ds = load_from_disk(dataset_path=str(shard_dir))\n",
    "    if nopack:\n",
    "        # in the DoReMi paper, we first padded to the context length then counted\n",
    "        # the number of chunks, and dynamically packed the examples\n",
    "        # together (possibly even from different domains)\n",
    "        num_tokens_in_curr_doc = 0\n",
    "        chunk_size = 2048\n",
    "        for ex in tqdm(ds):\n",
    "            toks = ex['input_ids']\n",
    "            sep_idxs = [i for i in range(len(toks)) if toks[i] == tokenizer.eos_token_id]\n",
    "            if len(sep_idxs) > 0:\n",
    "                prev_sep_idx = -1\n",
    "                for sep_idx in sep_idxs:\n",
    "                    num_tokens_in_curr_doc += sep_idx - prev_sep_idx - 1\n",
    "                    prev_sep_idx = sep_idx\n",
    "                    curr_count += math.ceil(num_tokens_in_curr_doc / chunk_size)\n",
    "                    num_tokens_in_curr_doc = 0\n",
    "                if prev_sep_idx != len(toks) - 1:\n",
    "                    num_tokens_in_curr_doc += len(toks) - prev_sep_idx - 1\n",
    "            else:\n",
    "                num_tokens_in_curr_doc += len(toks)\n",
    "        if num_tokens_in_curr_doc > 0:\n",
    "            curr_count += math.ceil(num_tokens_in_curr_doc / chunk_size)\n",
    "    else:\n",
    "        curr_count = len(ds)\n",
    "\n",
    "    return curr_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "domain_lens = defaultdict(int)\n",
    "for domain_dir in preprocessed_dir.iterdir():\n",
    "    print(\"Counting domain\", domain_dir.name)\n",
    "    counts = Parallel(n_jobs=30)(delayed(process_shard)(shard_dir) for shard_dir in domain_dir.iterdir())\n",
    "    domain_lens[domain_dir.name] = sum(counts)\n",
    "print(domain_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_lens = defaultdict(int)\n",
    "for domain_dir in preprocessed_dir.iterdir():\n",
    "    print(\"Counting domain\", domain_dir.name)\n",
    "    counts = Parallel(n_jobs=30)(delayed(process_shard)(shard_dir) for shard_dir in domain_dir.iterdir())\n",
    "    domain_lens[domain_dir.name] = sum(counts)\n",
    "print(domain_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = 0\n",
    "for domain in domain_lens.keys():\n",
    "    nums += domain_lens[domain]\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_epochs = {\n",
    "            \"RedPajamaCommonCrawl\": 1.0,\n",
    "            \"RedPajamaC4\": 1.0,\n",
    "            \"RedPajamaGithub\": 1.0,\n",
    "            \"RedPajamaWikipedia\": 1.0,\n",
    "            \"RedPajamaBook\": 1.0,\n",
    "            \"RedPajamaArXiv\": 1.0,\n",
    "            \"RedPajamaStackExchange\": 1.0\n",
    "        }\n",
    "domain_lens = {k: v * slim_epochs[k] for k,v in domain_lens.items()}\n",
    "print(domain_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renormalize domain_lens\n",
    "total_len = sum(domain_lens.values())\n",
    "domain_lens = {k: v / total_len for k, v in domain_lens.items()}\n",
    "print(\"Baseline domain weights:\", domain_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/wth/My_codes/doremi/data/multi_domain/train/train_allenai-WildChat-1m.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjsonlines\u001b[39;00m\n\u001b[1;32m      4\u001b[0m jsonl_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/wth/My_codes/doremi/data/multi_domain/train/train_allenai-WildChat-1m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjsonl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     items \u001b[38;5;241m=\u001b[39m jsonlines\u001b[38;5;241m.\u001b[39mReader(f)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "File \u001b[0;32m~/anaconda3/envs/doremi/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/wth/My_codes/doremi/data/multi_domain/train/train_allenai-WildChat-1m.jsonl'"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import jsonlines\n",
    "\n",
    "jsonl_path = \"/home/wth/My_codes/doremi/data/multi_domain/train/train_allenai-WildChat-1m.jsonl\"\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    items = jsonlines.Reader(f)\n",
    "    for item in items:\n",
    "        print(item)\n",
    "        print(item.keys())\n",
    "        print(item[\"text\"])\n",
    "        print(item[\"meta\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_Open-Orca-1million-gpt-4', 'train_allenai-WildChat-1m', 'train_alpaca_chat_turn2', 'train_alpaca_de_49963', 'train_alpaca_es_51942', 'train_alpaca_fr_55178', 'train_alpaca_gpt4', 'train_alpaca_it_51710', 'train_alpaca_ja_51999', 'train_alpaca_ko_49620', 'train_alpaca_pt_51759', 'train_alpaca_ru_29822', 'train_alpaca_zh_48818', 'train_jondurbin-airoboros-3.2', 'train_lmsys-chat-1m', 'train_share_gpt4', 'train_slimorca', 'train_teknium-GPTeacher-General-Instruct']\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_Open-Orca-1million-gpt-4\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_allenai-WildChat-1m\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_chat_turn2\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_de_49963\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_es_51942\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_fr_55178\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_gpt4\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_it_51710\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_ja_51999\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_ko_49620\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_pt_51759\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_ru_29822\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_alpaca_zh_48818\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_jondurbin-airoboros-3.2\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_lmsys-chat-1m\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_share_gpt4\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_slimorca\n",
      "/home/wth/My_codes/doremi/data/multi_domain/train/train_teknium-GPTeacher-General-Instruct\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "dataset_dir = \"/home/wth/My_codes/doremi/data/multi_domain/train\"\n",
    "dataset_dir = Path(dataset_dir)\n",
    "DOMAINS = list(sorted([str(domain_dir.name.split(\".j\")[0]) for domain_dir in dataset_dir.iterdir() if not str(domain_dir.name).endswith('txt')]))\n",
    "print(DOMAINS)\n",
    "for domain in DOMAINS:\n",
    "    domain_path = Path(dataset_dir) / domain\n",
    "    print(domain_path)\n",
    "    # os.mkdir(domain_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_file = \"/home/wth/My_codes/doremi/data/multi_domain/train/train_allenai-WildChat-1m/train_allenai-WildChat-1m.jsonl\"\n",
    "ds = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=data_file\n",
    ")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doremi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
